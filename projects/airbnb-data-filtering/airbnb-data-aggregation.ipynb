{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e54c1ccc-2b35-403c-99ea-8db4b20e9c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Aggregation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "listings = spark.read.csv(\"data/listings.csv.gz\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\",\n",
    "    quote='\"',\n",
    "    escape='\"',\n",
    "    multiLine=True,\n",
    "    mode=\"PERMISSIVE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1966fee9-beea-4439-bb07-38c0316a9a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-----+\n",
      "|property_type                  |count|\n",
      "+-------------------------------+-----+\n",
      "|Private room in loft           |46   |\n",
      "|Entire chalet                  |1    |\n",
      "|Entire rental unit             |14414|\n",
      "|Private room in minsu          |1    |\n",
      "|Shared room in hostel          |16   |\n",
      "|Private room in condo          |564  |\n",
      "|Room in boutique hotel         |47   |\n",
      "|Room in bed and breakfast      |6    |\n",
      "|Private room in casa particular|5    |\n",
      "|Entire cabin                   |1    |\n",
      "|Private room in nature lodge   |1    |\n",
      "|Entire guest suite             |21   |\n",
      "|Private room in home           |83   |\n",
      "|Entire place                   |10   |\n",
      "|Camper/RV                      |2    |\n",
      "|Tiny home                      |16   |\n",
      "|Entire vacation home           |272  |\n",
      "|Private room in camper/rv      |1    |\n",
      "|Private room in hostel         |25   |\n",
      "|Lighthouse                     |1    |\n",
      "+-------------------------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "listings \\\n",
    "    .groupby(listings.property_type) \\\n",
    "    .count() \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08ea4c47-6ebc-48ff-9bbe-a0f1010f1ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+\n",
      "|property_type                    |count|\n",
      "+---------------------------------+-----+\n",
      "|Entire rental unit               |14414|\n",
      "|Entire condo                     |4319 |\n",
      "|Private room in rental unit      |1480 |\n",
      "|Private room in condo            |564  |\n",
      "|Entire loft                      |444  |\n",
      "|Entire home                      |367  |\n",
      "|Entire vacation home             |272  |\n",
      "|Entire serviced apartment        |244  |\n",
      "|Private room in bed and breakfast|184  |\n",
      "|Room in hotel                    |146  |\n",
      "|Private room in home             |83   |\n",
      "|Room in boutique hotel           |47   |\n",
      "|Private room in loft             |46   |\n",
      "|Private room in guesthouse       |28   |\n",
      "|Private room in hostel           |25   |\n",
      "|Private room in vacation home    |23   |\n",
      "|Entire guest suite               |21   |\n",
      "|Private room in villa            |21   |\n",
      "|Shared room in rental unit       |19   |\n",
      "|Shared room in hostel            |16   |\n",
      "+---------------------------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "listings \\\n",
    "    .groupby(listings.property_type) \\\n",
    "    .agg(\n",
    "        F.count('property_type').alias('count')\n",
    "    ) \\\n",
    "    .orderBy('count', ascending=[False]) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fa2ff19-1527-4274-9ace-3ec38f019c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+---------------------------+\n",
      "|property_type                    |count|avg(review_scores_location)|\n",
      "+---------------------------------+-----+---------------------------+\n",
      "|Entire rental unit               |14414|4.717307471500011          |\n",
      "|Entire condo                     |4319 |4.749171656686611          |\n",
      "|Private room in rental unit      |1480 |4.669619410085644          |\n",
      "|Private room in condo            |564  |4.723645621181265          |\n",
      "|Entire loft                      |444  |4.711267942583738          |\n",
      "|Entire home                      |367  |4.7258688524590164         |\n",
      "|Entire vacation home             |272  |4.741479999999996          |\n",
      "|Entire serviced apartment        |244  |4.790233644859812          |\n",
      "|Private room in bed and breakfast|184  |4.626814159292035          |\n",
      "|Room in hotel                    |146  |4.547125000000004          |\n",
      "|Private room in home             |83   |4.68063492063492           |\n",
      "|Room in boutique hotel           |47   |4.796774193548386          |\n",
      "|Private room in loft             |46   |4.6838235294117645         |\n",
      "|Private room in guesthouse       |28   |4.278947368421052          |\n",
      "|Private room in hostel           |25   |4.841764705882352          |\n",
      "|Private room in vacation home    |23   |4.732000000000001          |\n",
      "|Entire guest suite               |21   |4.807                      |\n",
      "|Private room in villa            |21   |4.7299999999999995         |\n",
      "|Shared room in rental unit       |19   |4.847142857142857          |\n",
      "|Shared room in hostel            |16   |4.7631250000000005         |\n",
      "+---------------------------------+-----+---------------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "listings \\\n",
    "    .groupby(listings.property_type) \\\n",
    "    .agg(\n",
    "        F.count('property_type').alias('count'),\n",
    "        F.avg('review_scores_location')\n",
    "    ) \\\n",
    "    .orderBy('count', ascending=[False]) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67a8b944-f45c-4a9e-a78c-fc502968da1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "reviews = spark.read.csv(\"data/reviews.csv.gz\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\",\n",
    "    quote='\"',\n",
    "    escape='\"',\n",
    "    multiLine=True,\n",
    "    mode=\"PERMISSIVE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3c685cf-d25a-4f4c-86a0-a09d5c0a09a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructField('listing_id', LongType(), True)\n",
      "StructField('id', LongType(), True)\n",
      "StructField('date', DateType(), True)\n",
      "StructField('reviewer_id', IntegerType(), True)\n",
      "StructField('reviewer_name', StringType(), True)\n",
      "StructField('comments', StringType(), True)\n"
     ]
    }
   ],
   "source": [
    "for field in reviews.schema:\n",
    "    print(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "835d99d7-f482-4087-b029-794859c06300",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_reviews = listings.join(\n",
    "    reviews, listings.id == reviews.listing_id, how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfd19760-c27f-443f-b14d-6387bedd78b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2026-01-31 00:39:58.482\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[AMBIGUOUS_REFERENCE] Reference `id` is ambiguous, could be: [`id`, `id`]. SQLSTATE: 42704\", \"context\": {\"file\": \"java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"AMBIGUOUS_REFERENCE\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o245.agg.\\n: org.apache.spark.sql.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `id` is ambiguous, could be: [`id`, `id`]. SQLSTATE: 42704\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.ambiguousReferenceError(QueryCompilationErrors.scala:2232)\\n\\tat org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:356)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:164)\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpressionByPlanChildren$1(ColumnResolutionHelper.scala:435)\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$3(ColumnResolutionHelper.scala:147)\\n\\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:157)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:118)\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpression(ColumnResolutionHelper.scala:198)\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren(ColumnResolutionHelper.scala:442)\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren$(ColumnResolutionHelper.scala:427)\\n\\tat org.apache.spark.sql.catalyst.analysis.ResolveReferencesInAggregate.resolveExpressionByPlanChildren(ResolveReferencesInAggregate.scala:50)\\n\\tat org.apache.spark.sql.catalyst.analysis.ResolveReferencesInAggregate.$anonfun$apply$1(ResolveReferencesInAggregate.scala:63)\\n\\tat scala.collection.immutable.List.map(List.scala:236)\\n\\tat scala.collection.immutable.List.map(List.scala:79)\\n\\tat org.apache.spark.sql.catalyst.analysis.ResolveReferencesInAggregate.apply(ResolveReferencesInAggregate.scala:63)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$doApply$3.applyOrElse(Analyzer.scala:1677)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$doApply$3.applyOrElse(Analyzer.scala:1568)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.doApply(Analyzer.scala:1568)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1565)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1521)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\\n\\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\\n\\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\\n\\tat scala.collection.immutable.List.foldLeft(List.scala:79)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\\n\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:343)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:339)\\n\\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:339)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:236)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:90)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:54)\\n\\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:158)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:153)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:54)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.ambiguousReferenceError(QueryCompilationErrors.scala:2232)\\n\\t\\tat org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:356)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:164)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpressionByPlanChildren$1(ColumnResolutionHelper.scala:435)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$3(ColumnResolutionHelper.scala:147)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:157)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:118)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpression(ColumnResolutionHelper.scala:198)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren(ColumnResolutionHelper.scala:442)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren$(ColumnResolutionHelper.scala:427)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ResolveReferencesInAggregate.resolveExpressionByPlanChildren(ResolveReferencesInAggregate.scala:50)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ResolveReferencesInAggregate.$anonfun$apply$1(ResolveReferencesInAggregate.scala:63)\\n\\t\\tat scala.collection.immutable.List.map(List.scala:236)\\n\\t\\tat scala.collection.immutable.List.map(List.scala:79)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ResolveReferencesInAggregate.apply(ResolveReferencesInAggregate.scala:63)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$doApply$3.applyOrElse(Analyzer.scala:1677)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$doApply$3.applyOrElse(Analyzer.scala:1568)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.doApply(Analyzer.scala:1568)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1565)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1521)\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\\n\\t\\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\\n\\t\\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\\n\\t\\tat scala.collection.immutable.List.foldLeft(List.scala:79)\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:343)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:339)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:339)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:236)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 23 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/home/linuxbrew/.linuxbrew/Cellar/apache-spark/4.1.1/libexec/python/pyspark/errors/exceptions/captured.py\", \"line\": \"263\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/home/linuxbrew/.linuxbrew/Cellar/apache-spark/4.1.1/libexec/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[AMBIGUOUS_REFERENCE] Reference `id` is ambiguous, could be: [`id`, `id`]. SQLSTATE: 42704",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[43mlistings_reviews\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_reviews\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[32m      6\u001b[39m     .show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/linuxbrew/.linuxbrew/Cellar/apache-spark/4.1.1/libexec/python/pyspark/sql/group.py:190\u001b[39m, in \u001b[36mGroupedData.agg\u001b[39m\u001b[34m(self, *exprs)\u001b[39m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[33m\"\u001b[39m\u001b[33mall exprs should be Column\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    189\u001b[39m     exprs = cast(Tuple[Column, ...], exprs)\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jgd\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.session)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/linuxbrew/.linuxbrew/Cellar/apache-spark/4.1.1/libexec/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/linuxbrew/.linuxbrew/Cellar/apache-spark/4.1.1/libexec/python/pyspark/errors/exceptions/captured.py:269\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    265\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [AMBIGUOUS_REFERENCE] Reference `id` is ambiguous, could be: [`id`, `id`]. SQLSTATE: 42704"
     ]
    }
   ],
   "source": [
    "listings_reviews \\\n",
    "    .groupBy('id') \\\n",
    "    .agg(\n",
    "        F.count('id').alias('num_reviews')\n",
    "    ) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8f6a94d-fb7d-48a1-bde0-f808fe49d0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------------------------------+-----------+\n",
      "|id      |name                                             |num_reviews|\n",
      "+--------+-------------------------------------------------+-----------+\n",
      "|833602  |Beautiful flat in Milano center !!!              |1395       |\n",
      "|5098399 |Sant'Ambrogio Central&Quiet                      |1143       |\n",
      "|1482235 |Flat in Milan's Shopping District                |1132       |\n",
      "|15961311|[Navigli-Tortona] Modern flat with Wi-Fi and AC  |1123       |\n",
      "|2701066 |Room with a beautiful garden                     |1118       |\n",
      "|6271080 |Apartment in the heart of Milan                  |1093       |\n",
      "|20761132|MilanRentals - Soperga- Milan Central Station    |1065       |\n",
      "|5257587 |Naviglio : The sound of Silence                  |1041       |\n",
      "|19221782|DUOMO Luxury with Terrace in Prestigious Building|1011       |\n",
      "|768969  |Central Milan Wifi Flat & Breakfast              |994        |\n",
      "|6350702 |Central Station charming flat, wifi              |986        |\n",
      "|3765731 |Parsifal Apartment                               |984        |\n",
      "|16538780|MilanRentals - Tarra                             |978        |\n",
      "|16161609|Piccolo gioiello in Brera/Luxury flat in Brera   |973        |\n",
      "|1605608 |Grazioso e luminoso monolocale Dergano M3 gialla |972        |\n",
      "|2389898 |Loft in Fashion District                         |969        |\n",
      "|10713337|Single room in Corso Como area                   |950        |\n",
      "|24515432|Piccola Mansarda in Corso Como                   |931        |\n",
      "|3535342 |Elegant & Prestigious Apartment in Milan Center  |928        |\n",
      "|4426682 |Charming Milanese Apartment                      |925        |\n",
      "+--------+-------------------------------------------------+-----------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "reviews_per_listing = listings_reviews \\\n",
    "    .groupBy(listings.id, listings.name) \\\n",
    "    .agg(\n",
    "        F.count(reviews.id).alias('num_reviews')\n",
    "    ) \\\n",
    "    .orderBy('num_reviews', ascending=False) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67874d54-2d97-47c9-be49-8a92745e4d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
